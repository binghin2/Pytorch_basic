{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4b6303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP와 binary classification 예제\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47825b5",
   "metadata": {},
   "source": [
    "# 데이터 모으기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69104fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case1\n",
    "N = 20 # 총 데이터 수\n",
    "random0 = torch.randn(int(N/2), 1)  # 이건 평균 0, 표준편차 1인 정규분포에서 뽑은 난수 10개 10 x 1 크기의 텐서\n",
    "random5 = torch.randn(int(N/2), 1) + 5 # 이건 평균 5, 표준편차 1인 정규분포에서 뽑은 난수 10개 10 x 1 크기의 텐서\n",
    "class1_data = torch.hstack([random0,random5]) # hstack => 수평방향으로 텐서 합치기 20 x 2 크기의 텐서로 나옴\n",
    "class2_data = torch.hstack([random5,random0])\n",
    "class1_label = torch.ones(int(N/2),1)\n",
    "class2_label = torch.zeros(int(N/2),1)\n",
    "X = torch.vstack([class1_data, class2_data])\n",
    "Y = torch.vstack([class1_label, class2_label]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c690b680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(class1_data[:,0], class1_data[:,1], 'o')\n",
    "plt.plot(class2_data[:,0], class2_data[:,1], 'ro')\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.grid()\n",
    "# 위쪽이 label 1, 아래쪽이 label 0임\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec08e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classification 하기\n",
    "# 1. 데이터 모으기 이건 위에서 함\n",
    "# 2. 모델 만들기\n",
    "# 3. 모델 학습\n",
    "# 4. 모델 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67497aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 모델 만들기\n",
    "from torch import nn \n",
    "\n",
    "class MLP(nn.Module): # nn.Module을 상속받음 nn.Module: 파이토치에서 신경망 모델을 만들 때 사용하는 기본 클래스\n",
    "    def __init__(self): # self는 클래스의 인스턴스를 가리키는 매개변수 그리고 __init__는 생성자 메서드\n",
    "        super().__init__() # 이건 부모 클래스의 init 함수를 호출하는 것 super()는 부모 클래스를 참조하는 함수\n",
    "\n",
    "        # case 1, plain MLP\n",
    "        self.linear = nn.Sequential(nn.Linear(2, 100), # 입력층 2개, 은닉층 100개 hyperparameter는 내가 정함 100개는 나중에 바꿔가면서 함\n",
    "                                    nn.Sigmoid(), # 활성화 함수\n",
    "                                    nn.Linear(100, 1), # 출력층 1개\n",
    "                                    nn.Sigmoid()) # 활성화 함수\n",
    "        # nn.Sequential은 여러 층을 순차적으로 쌓을 때 사용하는 함수\n",
    "        # nn.Linear(2,100)은 입력이 2차원이고 출력이 100차원인 선형 변환 층을 의미\n",
    "        # nn.Linear(100,1)은 입력이 100차원이고 출력이 1차원인 선형 변환 층을 의미\n",
    "        # 이 모델은 2개에서 100개로 변환한 후 다시 1개로 변환하는 구조임\n",
    "\n",
    "        # # case 1, very simple MLP\n",
    "        # self.linear = nn.Sequential(nn.Linear(2, 1),\n",
    "        #                             nn.Sigmoid())\n",
    "        # 이 모델은 2개에서 1개로 바로 변환하는 구조임\n",
    "        # # case 1, Deep MLP\n",
    "        # self.linear = nn.Sequential(nn.Linear(2, 100),\n",
    "        #                             nn.Sigmoid(),\n",
    "        #                             nn.Linear(100, 100),\n",
    "        #                             nn.Sigmoid(),\n",
    "        #                             nn.Linear(100, 100),\n",
    "        #                             nn.Sigmoid(),\n",
    "        #                             nn.Linear(100, 1),\n",
    "        #                             nn.Sigmoid())\n",
    "        # 이 모델은 2개에서 100개, 100개, 100개, 1개로 변환 \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "        # 이 def forward(self, x):는 모델의 순전파를 정의하는 메서드임 무조건 있어야 함\n",
    "# 따라서 MLP 클래스는 \n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__() 와 \n",
    "# -------------중간 코드-------------\n",
    "# def forward(self, x):\n",
    "# x = self.linear(x)\n",
    "# return x \n",
    "# 가 반드시 있어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e518168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (linear): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "model = MLP()\n",
    "print(model)\n",
    "\n",
    "model.eval()\n",
    "print(model(torch.randn(5,2)).shape)\n",
    "# torch.randn(5,2)가 의미하는건 5개의 샘플에 대해 2개의 특성을 가진 랜덤한 입력 데이터임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50bdf7",
   "metadata": {},
   "source": [
    "# 3. 모델 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d9dd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 0.0252\n",
      "-\n",
      "Epoch: 2, train loss: 0.0250\n",
      "-\n",
      "Epoch: 3, train loss: 0.0248\n",
      "-\n",
      "Epoch: 4, train loss: 0.0246\n",
      "-\n",
      "Epoch: 5, train loss: 0.0243\n",
      "-\n",
      "Epoch: 6, train loss: 0.0241\n",
      "-\n",
      "Epoch: 7, train loss: 0.0239\n",
      "-\n",
      "Epoch: 8, train loss: 0.0237\n",
      "-\n",
      "Epoch: 9, train loss: 0.0236\n",
      "-\n",
      "Epoch: 10, train loss: 0.0234\n",
      "-\n",
      "Epoch: 11, train loss: 0.0232\n",
      "-\n",
      "Epoch: 12, train loss: 0.0230\n",
      "-\n",
      "Epoch: 13, train loss: 0.0228\n",
      "-\n",
      "Epoch: 14, train loss: 0.0226\n",
      "-\n",
      "Epoch: 15, train loss: 0.0225\n",
      "-\n",
      "Epoch: 16, train loss: 0.0223\n",
      "-\n",
      "Epoch: 17, train loss: 0.0221\n",
      "-\n",
      "Epoch: 18, train loss: 0.0219\n",
      "-\n",
      "Epoch: 19, train loss: 0.0218\n",
      "-\n",
      "Epoch: 20, train loss: 0.0216\n",
      "-\n",
      "Epoch: 21, train loss: 0.0215\n",
      "-\n",
      "Epoch: 22, train loss: 0.0213\n",
      "-\n",
      "Epoch: 23, train loss: 0.0211\n",
      "-\n",
      "Epoch: 24, train loss: 0.0210\n",
      "-\n",
      "Epoch: 25, train loss: 0.0208\n",
      "-\n",
      "Epoch: 26, train loss: 0.0207\n",
      "-\n",
      "Epoch: 27, train loss: 0.0206\n",
      "-\n",
      "Epoch: 28, train loss: 0.0204\n",
      "-\n",
      "Epoch: 29, train loss: 0.0203\n",
      "-\n",
      "Epoch: 30, train loss: 0.0201\n",
      "-\n",
      "Epoch: 31, train loss: 0.0200\n",
      "-\n",
      "Epoch: 32, train loss: 0.0199\n",
      "-\n",
      "Epoch: 33, train loss: 0.0197\n",
      "-\n",
      "Epoch: 34, train loss: 0.0196\n",
      "-\n",
      "Epoch: 35, train loss: 0.0195\n",
      "-\n",
      "Epoch: 36, train loss: 0.0193\n",
      "-\n",
      "Epoch: 37, train loss: 0.0192\n",
      "-\n",
      "Epoch: 38, train loss: 0.0191\n",
      "-\n",
      "Epoch: 39, train loss: 0.0190\n",
      "-\n",
      "Epoch: 40, train loss: 0.0188\n",
      "-\n",
      "Epoch: 41, train loss: 0.0187\n",
      "-\n",
      "Epoch: 42, train loss: 0.0186\n",
      "-\n",
      "Epoch: 43, train loss: 0.0185\n",
      "-\n",
      "Epoch: 44, train loss: 0.0184\n",
      "-\n",
      "Epoch: 45, train loss: 0.0183\n",
      "-\n",
      "Epoch: 46, train loss: 0.0181\n",
      "-\n",
      "Epoch: 47, train loss: 0.0180\n",
      "-\n",
      "Epoch: 48, train loss: 0.0179\n",
      "-\n",
      "Epoch: 49, train loss: 0.0178\n",
      "-\n",
      "Epoch: 50, train loss: 0.0177\n",
      "-\n",
      "Epoch: 51, train loss: 0.0176\n",
      "-\n",
      "Epoch: 52, train loss: 0.0175\n",
      "-\n",
      "Epoch: 53, train loss: 0.0174\n",
      "-\n",
      "Epoch: 54, train loss: 0.0173\n",
      "-\n",
      "Epoch: 55, train loss: 0.0172\n",
      "-\n",
      "Epoch: 56, train loss: 0.0171\n",
      "-\n",
      "Epoch: 57, train loss: 0.0170\n",
      "-\n",
      "Epoch: 58, train loss: 0.0169\n",
      "-\n",
      "Epoch: 59, train loss: 0.0168\n",
      "-\n",
      "Epoch: 60, train loss: 0.0167\n",
      "-\n",
      "Epoch: 61, train loss: 0.0166\n",
      "-\n",
      "Epoch: 62, train loss: 0.0165\n",
      "-\n",
      "Epoch: 63, train loss: 0.0164\n",
      "-\n",
      "Epoch: 64, train loss: 0.0164\n",
      "-\n",
      "Epoch: 65, train loss: 0.0163\n",
      "-\n",
      "Epoch: 66, train loss: 0.0162\n",
      "-\n",
      "Epoch: 67, train loss: 0.0161\n",
      "-\n",
      "Epoch: 68, train loss: 0.0160\n",
      "-\n",
      "Epoch: 69, train loss: 0.0159\n",
      "-\n",
      "Epoch: 70, train loss: 0.0158\n",
      "-\n",
      "Epoch: 71, train loss: 0.0158\n",
      "-\n",
      "Epoch: 72, train loss: 0.0157\n",
      "-\n",
      "Epoch: 73, train loss: 0.0156\n",
      "-\n",
      "Epoch: 74, train loss: 0.0155\n",
      "-\n",
      "Epoch: 75, train loss: 0.0154\n",
      "-\n",
      "Epoch: 76, train loss: 0.0154\n",
      "-\n",
      "Epoch: 77, train loss: 0.0153\n",
      "-\n",
      "Epoch: 78, train loss: 0.0152\n",
      "-\n",
      "Epoch: 79, train loss: 0.0151\n",
      "-\n",
      "Epoch: 80, train loss: 0.0150\n",
      "-\n",
      "Epoch: 81, train loss: 0.0150\n",
      "-\n",
      "Epoch: 82, train loss: 0.0149\n",
      "-\n",
      "Epoch: 83, train loss: 0.0148\n",
      "-\n",
      "Epoch: 84, train loss: 0.0148\n",
      "-\n",
      "Epoch: 85, train loss: 0.0147\n",
      "-\n",
      "Epoch: 86, train loss: 0.0146\n",
      "-\n",
      "Epoch: 87, train loss: 0.0145\n",
      "-\n",
      "Epoch: 88, train loss: 0.0145\n",
      "-\n",
      "Epoch: 89, train loss: 0.0144\n",
      "-\n",
      "Epoch: 90, train loss: 0.0143\n",
      "-\n",
      "Epoch: 91, train loss: 0.0143\n",
      "-\n",
      "Epoch: 92, train loss: 0.0142\n",
      "-\n",
      "Epoch: 93, train loss: 0.0141\n",
      "-\n",
      "Epoch: 94, train loss: 0.0141\n",
      "-\n",
      "Epoch: 95, train loss: 0.0140\n",
      "-\n",
      "Epoch: 96, train loss: 0.0139\n",
      "-\n",
      "Epoch: 97, train loss: 0.0139\n",
      "-\n",
      "Epoch: 98, train loss: 0.0138\n",
      "-\n",
      "Epoch: 99, train loss: 0.0137\n",
      "-\n",
      "Epoch: 100, train loss: 0.0137\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "from torch import optim # 옵티마이저 불러오기\n",
    "\n",
    "LR = 1e-1 # learning rate 설정 1e-1 = 0.1\n",
    "# 1e - 1 = 1 * 10^(-1) = 0.1\n",
    "EPOCH = 100 \n",
    "optimizer = optim.SGD(model.parameters(), lr=LR) # SGD 옵티마이저 사용 model.parameters()는 모델의 모든 학습 가능한 매개변수를 반환 lr은 learning rate 설정\n",
    "criterion = nn.BCELoss() # 이진 분류에서 사용하는 손실 함수 Binary Cross Entropy Loss 이것도 내가 정함\n",
    "\n",
    "loss_history = [] # 손실 값을 저장할 리스트\n",
    "grad_history = [] # 그래디언트 값을 저장할 리스트\n",
    "update_size_history = [] # 업데이트 크기를 저장할 리스트\n",
    "\n",
    "model.train() # 모델을 학습 모드로 설정\n",
    "for ep in range(EPOCH):\n",
    "    # inference\n",
    "    y_hat = model(X) # 모델에 입력 데이터 X를 넣어 예측값 y_hat을 얻음\n",
    "    # loss 계산\n",
    "    loss = criterion(y_hat, Y)\n",
    "    optimizer.zero_grad() # 옵티마이저의 그래디언트를 초기화\n",
    "    loss.backward() # 역전파 수행하여 그래디언트 계산\n",
    "    optimizer.step() # 옵티마이저를 사용하여 모델의 매개변수 업데이트\n",
    "    #print loss\n",
    "    loss_history += [loss.item()] # loss.item()은 텐서에서 스칼라 값을 추출하는 메서드\n",
    "    print(f\"Epoch: {ep+1}, train loss: {loss.item():.4f}\")\n",
    "    print(\"-\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e3e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0137, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "tensor(0.0137, grad_fn=<DivBackward0>)\n",
      "tensor(100.)\n"
     ]
    }
   ],
   "source": [
    "print(criterion(y_hat, Y)) # criterion(y_hat, Y)는 예측값 y_hat과 실제값 Y 사이의 손실 값을 계산함\n",
    "print(torch.sum(-torch.log( y_hat**Y * (1 - y_hat)**(1 - Y)))/ N) # 수식으로 직접 계산한 binary cross entropy loss 값\n",
    "print(criterion(torch.tensor([0.]), torch.tensor([1.]))) # 이건 예측값이 0이고 실제값이 1일 때의 손실 값을 계산 100이나오는 이유는 -log(0) = 무한대 이므로 아주 큰 값으로 표현됨"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
